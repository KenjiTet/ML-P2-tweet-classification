{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1978126,"status":"ok","timestamp":1702581554362,"user":{"displayName":"medimed","userId":"13680012807843858858"},"user_tz":-60},"id":"x6DwFa1YAS5_","outputId":"56467e25-8036-45ff-9489-28ef4df7882d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"name":"stderr","output_type":"stream","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"name":"stdout","output_type":"stream","text":["0\n","1\n","2\n","3\n","4\n","Accuracy: 0.881325\n"]}],"source":["import os\n","import csv\n","import numpy as np\n","import pandas as pd\n","import torch\n","from google.colab import drive\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from torch.cuda.amp import autocast, GradScaler\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","def load_data(folder_path):\n","    small_neg_path = os.path.join(folder_path, \"train_neg.txt\")\n","    small_pos_path = os.path.join(folder_path, \"train_pos.txt\")\n","    test_path = os.path.join(folder_path, \"test_data.txt\")\n","\n","    with open(small_neg_path, 'r') as file:\n","        lines_neg = file.readlines()\n","    with open(small_pos_path, 'r') as file:\n","        lines_pos = file.readlines()\n","    with open(test_path, 'r') as file:\n","        lines_test = file.readlines()\n","\n","    small_neg_df = pd.DataFrame({'Tweets': lines_neg, 'Sentiment': -1})\n","    small_pos_df = pd.DataFrame({'Tweets': lines_pos, 'Sentiment': 1})\n","    test_df= pd.DataFrame({'Tweets': lines_test})\n","    combined_df = pd.concat([small_neg_df, small_pos_df], ignore_index=True)\n","\n","    return combined_df, test_df\n","\n","def create_csv_submission(ids, y_pred, name):\n","    \"\"\"\n","    This function creates a csv file named 'name' in the format required for a submission in Kaggle or AIcrowd.\n","    The file will contain two columns the first with 'ids' and the second with 'y_pred'.\n","    y_pred must be a list or np.array of 1 and -1 otherwise the function will raise a ValueError.\n","\n","    Args:\n","        ids (list,np.array): indices\n","        y_pred (list,np.array): predictions on data correspondent to indices\n","        name (str): name of the file to be created\n","    \"\"\"\n","    # Check that y_pred only contains -1 and 1\n","    if not all(i in [-1, 1] for i in y_pred):\n","        raise ValueError(\"y_pred can only contain values -1, 1\")\n","\n","    with open(name, \"w\", newline=\"\") as csvfile:\n","        fieldnames = [\"Id\", \"Prediction\"]\n","        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n","        writer.writeheader()\n","        for r1, r2 in zip(ids, y_pred):\n","            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n","\n","def tokenize_tweets(tokenizer, tweet):\n","    tokens = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n","    return tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze()\n","\n","class TweetsDataset(Dataset):\n","    def __init__(self, tweets, sentiments, tokenizer):\n","        self.tweets = tweets\n","        self.sentiments = sentiments\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, idx):\n","        tweet, sentiment = self.tweets[idx], self.sentiments[idx]\n","        input_ids, attention_mask = tokenize_tweets(self.tokenizer, tweet)\n","        sentiment = 1 if sentiment == 1 else 0\n","        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'sentiment': sentiment}\n","\n","def train_model(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=sentiment)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n","            true_labels.extend(sentiment.cpu().numpy())\n","\n","    return true_labels, predictions\n","\n","def predict_sentiment(model, tokenizer, tweet, device):\n","    model.eval()\n","    tokens = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n","    input_ids, attention_mask = tokens['input_ids'].to(device), tokens['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","    return predicted_class\n","\n","def main():\n","    # File path in Google Drive\n","    folder_path = '/content/drive/MyDrive/Colab Notebooks'\n","\n","    # Load pre-trained BERT model and tokenizer\n","    model_name = 'bert-base-uncased'\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","    model = BertForSequenceClassification.from_pretrained(model_name).to(device)\n","\n","    # Tokenize and preprocess tweets using BERT\n","    combined_df, test_df = load_data(folder_path)\n","\n","    # Split the data into train and test sets\n","    train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=42)\n","\n","    # Create DataLoader for training set\n","    train_dataset = TweetsDataset(train_df['Tweets'].values, train_df['Sentiment'].values, tokenizer)\n","    train_loader = DataLoader(train_dataset, batch_size=20, shuffle=True)\n","\n","    # Create DataLoader for test set\n","    test_dataset = TweetsDataset(test_df['Tweets'].values, test_df['Sentiment'].values, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=20, shuffle=False)\n","\n","    # Training loop with optimizations\n","    scaler = GradScaler()\n","\n","    epochs = 5\n","    accumulation_steps = 4  # Adjust this based on GPU memory\n","    total_steps = len(train_loader) * epochs // accumulation_steps\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        print(epoch)\n","        for i, batch in enumerate(train_loader):\n","            input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","\n","            with autocast():\n","                input_ids, attention_mask, sentiment = input_ids.to(device), attention_mask.to(device), sentiment.to(device)\n","                outputs = model(input_ids, attention_mask=attention_mask, labels=sentiment)\n","                loss = outputs.loss / accumulation_steps  # Scale the loss\n","\n","            scaler.scale(loss).backward()\n","\n","            if (i + 1) % accumulation_steps == 0 or i == len(train_loader) - 1:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                scheduler.step()\n","\n","    # Evaluation\n","    true_labels, predictions = evaluate_model(model, test_loader, device)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(true_labels, predictions)\n","    print(f\"Accuracy: {accuracy}\")\n","\n","    # Predict and create submission\n","    test_df['Predicted_Sentiment'] = test_df['Tweets'].apply(lambda tweet: predict_sentiment(model, tokenizer, tweet, device))\n","    sentiment_mapping = {0: -1, 1: 1}\n","    test_df['Predicted_Sentiment'] = test_df['Predicted_Sentiment'].map(sentiment_mapping)\n","    y_pred = test_df['Predicted_Sentiment'].values\n","\n","    ids = np.arange(1, len(y_pred) + 1)\n","    submission_path = os.path.join(folder_path, \"submission.csv\")\n","    create_csv_submission(ids, y_pred, submission_path)\n","\n","if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"executionInfo":{"elapsed":302,"status":"error","timestamp":1702582321559,"user":{"displayName":"medimed","userId":"13680012807843858858"},"user_tz":-60},"id":"qSpNiKmVzmjq","outputId":"167f1cba-e7b9-4590-c130-047df7467772"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-dded343a3069>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentiment_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-23-dded343a3069>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(tweet)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msentiment_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentiment_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["test_df['Predicted_Sentiment'] = test_df['Tweets'].apply(lambda tweet: predict_sentiment(model, tokenizer, tweet, device))\n","sentiment_mapping = {0: -1, 1: 1}\n","test_df['Predicted_Sentiment'] = test_df['Predicted_Sentiment'].map(sentiment_mapping)\n","y_pred = test_df['Predicted_Sentiment'].values"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":391},"executionInfo":{"elapsed":665,"status":"error","timestamp":1702575397380,"user":{"displayName":"medimed","userId":"13680012807843858858"},"user_tz":-60},"id":"uqZFpNcGZZVY","outputId":"792ff905-6130-4caf-b918-2210f64fa2c8"},"outputs":[{"ename":"NameError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-bf5301ab41e5>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# Apply the function to the 'Tweets' column in test_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Predicted_Sentiment'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Tweets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0msentiment_mapping\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4769\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[0;32m-> 4771\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSeriesApply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4772\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4773\u001b[0m     def _reduce(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;31m# self.f is Callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1123\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1125\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m                 mapped = lib.map_infer(\n\u001b[0m\u001b[1;32m   1175\u001b[0m                     \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m                     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","\u001b[0;32m<ipython-input-18-bf5301ab41e5>\u001b[0m in \u001b[0;36mpredict_sentiment\u001b[0;34m(tweet)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Create a function to preprocess tweets and get predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpredict_sentiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'max_length'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_ids'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'attention_mask'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["# File path in Google Drive\n","folder_path = '/content/drive/MyDrive/Colab Notebooks'\n","test_path = os.path.join(folder_path, \"test_data.txt\")\n","\n","#create small data dataframe\n","with open(test_path, 'r') as file:\n","    lines_test = file.readlines()\n","\n","test_df= pd.DataFrame({'Tweets': lines_test})\n","\n","# Create a function to preprocess tweets and get predictions\n","def predict_sentiment(tweet):\n","    model.eval()\n","    tokens = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n","    input_ids, attention_mask = tokens['input_ids'].to(device), tokens['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","    return predicted_class\n","\n","# Apply the function to the 'Tweets' column in test_df\n","test_df['Predicted_Sentiment'] = test_df['Tweets'].apply(predict_sentiment)\n","\n","sentiment_mapping = {0: -1, 1: 1}\n","\n","# Map predicted sentiments\n","test_df['Predicted_Sentiment'] = test_df['Predicted_Sentiment'].map(sentiment_mapping)\n","\n","# 'Mapped_Predicted_Sentiment' column now contains the mapped sentiments\n","y_pred = test_df['Predicted_Sentiment'].values\n","\n","def create_csv_submission(ids, y_pred, name):\n","    \"\"\"\n","    This function creates a csv file named 'name' in the format required for a submission in Kaggle or AIcrowd.\n","    The file will contain two columns the first with 'ids' and the second with 'y_pred'.\n","    y_pred must be a list or np.array of 1 and -1 otherwise the function will raise a ValueError.\n","\n","    Args:\n","        ids (list,np.array): indices\n","        y_pred (list,np.array): predictions on data correspondent to indices\n","        name (str): name of the file to be created\n","    \"\"\"\n","    # Check that y_pred only contains -1 and 1\n","    if not all(i in [-1, 1] for i in y_pred):\n","        raise ValueError(\"y_pred can only contain values -1, 1\")\n","\n","    with open(name, \"w\", newline=\"\") as csvfile:\n","        fieldnames = [\"Id\", \"Prediction\"]\n","        writer = csv.DictWriter(csvfile, delimiter=\",\", fieldnames=fieldnames)\n","        writer.writeheader()\n","        for r1, r2 in zip(ids, y_pred):\n","            writer.writerow({\"Id\": int(r1), \"Prediction\": int(r2)})\n","\n","#!!!!don't forget to change the name of the model!!!!\n","ids=np.arange(1,len(y_pred)+1)\n","submission_path = os.path.join(folder_path, \"submission.csv\")\n","create_csv_submission(ids, y_pred, submission_path)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMiYFaDFEYJEJFYCJQgoqKV","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
