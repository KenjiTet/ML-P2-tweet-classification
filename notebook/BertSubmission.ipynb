{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Training the Bert model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import os\n","import numpy as np\n","import pandas as pd\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertForSequenceClassification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","from torch.cuda.amp import autocast, GradScaler\n","\n","\n","def load_data(folder_path):\n","    #get small data path\n","    small_neg_path = os.path.join(folder_path, \"twitter-datasets\\\\train_neg_full.txt\")\n","    small_pos_path = os.path.join(folder_path, \"twitter-datasets\\\\train_pos_full.txt\")\n","    test_path = os.path.join(folder_path, \"twitter-datasets\\\\test_data.txt\")\n","\n","    #create small data dataframe\n","    with open(small_neg_path, 'r') as file:\n","        lines_neg = file.readlines()\n","    with open(small_pos_path, 'r') as file:\n","        lines_pos = file.readlines()\n","    with open(test_path, 'r') as file:\n","        lines_test = file.readlines()\n","        lines_test = [s.split(',', 1)[1] for s in lines_test]\n","\n","    small_neg_df = pd.DataFrame({'Tweets': lines_neg, 'Sentiment': -1})\n","    small_pos_df = pd.DataFrame({'Tweets': lines_pos, 'Sentiment': 1})\n","    test_df= pd.DataFrame({'Tweets': lines_test})\n","    test_df['Tweets'] = test_df['Tweets'].str.rstrip('\\n')\n","    combined_df = pd.concat([small_neg_df, small_pos_df], ignore_index=True).sample(frac=1).reset_index(drop=True)\n","    combined_df['Tweets'] = combined_df['Tweets'].str.rstrip('\\n')\n","    return combined_df, test_df\n","\n","\n","def tokenize_tweets(tokenizer, tweet):\n","    tokens = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n","    return tokens['input_ids'].squeeze(), tokens['attention_mask'].squeeze()\n","\n","class TweetsDataset(Dataset):\n","    def __init__(self, tweets, sentiments, tokenizer):\n","        self.tweets = tweets\n","        self.sentiments = sentiments\n","        self.tokenizer = tokenizer\n","\n","    def __len__(self):\n","        return len(self.tweets)\n","\n","    def __getitem__(self, idx):\n","        tweet, sentiment = self.tweets[idx], self.sentiments[idx]\n","        input_ids, attention_mask = tokenize_tweets(self.tokenizer, tweet)\n","        sentiment = 1 if sentiment == 1 else 0\n","        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'sentiment': sentiment}\n","\n","def train_model(model, train_loader, optimizer, criterion, device):\n","    model.train()\n","    for batch in train_loader:\n","        input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask=attention_mask, labels=sentiment)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","\n","def evaluate_model(model, test_loader, device):\n","    model.eval()\n","    predictions = []\n","    true_labels = []\n","\n","    with torch.no_grad():\n","        for batch in test_loader:\n","            input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","            outputs = model(input_ids, attention_mask=attention_mask)\n","            logits = outputs.logits\n","            predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n","            true_labels.extend(sentiment.cpu().numpy())\n","\n","    return true_labels, predictions\n","\n","\n","def predict_sentiment(model, tokenizer, tweet, device):\n","    model.eval()\n","    tokens = tokenizer.encode_plus(tweet, add_special_tokens=True, max_length=128, padding='max_length', return_tensors='pt')\n","    input_ids, attention_mask = tokens['input_ids'].to(device), tokens['attention_mask'].to(device)\n","\n","    with torch.no_grad():\n","        outputs = model(input_ids, attention_mask=attention_mask)\n","        logits = outputs.logits\n","\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","    return predicted_class\n","\n","def main():\n","    # File path in Google Drive\n","    folder_path = os.path.dirname(os.getcwd())\n","\n","    # Load pre-trained BERT model and tokenizer\n","    model_name = 'bert-base-uncased'\n","    tokenizer = BertTokenizer.from_pretrained(model_name)\n","    model = BertForSequenceClassification.from_pretrained(model_name).to(device)\n","\n","    # Tokenize and preprocess tweets using BERT\n","    combined_df, test_df = load_data(folder_path)\n","\n","    # Split the data into train and test sets\n","    train_df, test_df = train_test_split(combined_df, test_size=0.2, random_state=2)\n","\n","    # Create DataLoader for training set\n","    train_dataset = TweetsDataset(train_df['Tweets'].values, train_df['Sentiment'].values, tokenizer)\n","    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","\n","    # Create DataLoader for test set\n","    test_dataset = TweetsDataset(test_df['Tweets'].values, test_df['Sentiment'].values, tokenizer)\n","    test_loader = DataLoader(test_dataset, batch_size=40, shuffle=False)\n","\n","    # Training loop with optimizations\n","    scaler = GradScaler()\n","\n","    epochs = 5\n","    accumulation_steps = 5  # Adjust this based on GPU memory\n","    total_steps = len(train_loader) * epochs // accumulation_steps\n","\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n","    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, total_steps)\n","\n","    for epoch in range(epochs):\n","        model.train()\n","        print(epoch)\n","        for i, batch in enumerate(train_loader):\n","            input_ids, attention_mask, sentiment = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['sentiment'].to(device)\n","\n","            with autocast():\n","                outputs = model(input_ids, attention_mask=attention_mask, labels=sentiment)\n","                loss = outputs.loss / accumulation_steps  # Scale the loss\n","\n","            scaler.scale(loss).backward()\n","\n","            if (i + 1) % accumulation_steps == 0 or i == len(train_loader) - 1:\n","                scaler.step(optimizer)\n","                scaler.update()\n","                optimizer.zero_grad()\n","                scheduler.step()\n","\n","    # Evaluation\n","    true_labels, predictions = evaluate_model(model, test_loader, device)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(true_labels, predictions)\n","    print(f\"Accuracy: {accuracy}\")\n","\n","    # Save the model\n","    model_save_path = os.path.join(folder_path, 'saved_model')\n","    model.save_pretrained(model_save_path)\n","    tokenizer.save_pretrained(model_save_path)\n","\n","if __name__ == \"__main__\":\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    main()"]},{"cell_type":"markdown","metadata":{},"source":["# Loading and evaluating the Bert model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load the model\n","folder_path = os.path.dirname(os.getcwd())\n","combined_df, test_df = load_data(folder_path)\n","model_save_path = os.path.join(folder_path, 'models\\\\saved_model')\n","loaded_model = BertForSequenceClassification.from_pretrained(model_save_path).to(device)\n","loaded_tokenizer = BertTokenizer.from_pretrained(model_save_path)\n","test_df['Predicted_Sentiment'] = test_df['Tweets'].apply(lambda tweet: predict_sentiment(loaded_model, loaded_tokenizer, tweet, device))\n","sentiment_mapping = {0: -1, 1: 1}\n","test_df['Predicted_Sentiment'] = test_df['Predicted_Sentiment'].map(sentiment_mapping)\n","y_pred = test_df['Predicted_Sentiment'].values\n","ids = np.arange(1, len(y_pred) + 1)\n","submission_path = os.path.join(folder_path, \"predictions\\\\BerSubmission.csv\")\n","submission_df = pd.DataFrame({'Id': ids, 'Prediction': y_pred})\n","submission_df.to_csv(submission_path, index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMiYFaDFEYJEJFYCJQgoqKV","gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":0}
